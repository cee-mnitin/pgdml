{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection\n",
    "\n",
    "### Problem Statement\n",
    "The problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.\n",
    "\n",
    "In this project, you will analyse customer-level data that has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group. \n",
    "\n",
    "### Business problem overview\n",
    "For many banks, retaining high profitable customers is the number one business goal. Banking fraud, however, poses a significant threat to this goal for different banks. In terms of substantial financial losses, trust and credibility, this is a concerning issue to both banks and customers alike.\n",
    "\n",
    "It has been estimated by Nilson Report that by 2020, banking frauds would account for $30 billion worldwide. With the rise in digital payment channels, the number of fraudulent transactions is also increasing in new and different ways. \n",
    "\n",
    "\n",
    "In the banking industry, credit card fraud detection using machine learning is not only a trend but a necessity for them to put proactive monitoring and fraud prevention mechanisms in place. Machine learning is helping these institutions to reduce time-consuming manual reviews, costly chargebacks and fees as well as denials of legitimate transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# import machine learning and stats libraries:\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, f1_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "# Import:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#!pip install xgboost imblearn scikit-optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Row and Column Count\n",
    "print(df.shape)\n",
    "# Check distribution of data:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Column Names, Types, Counts, Null counts\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe the different feature type present in the data\n",
    "\n",
    "#Check the fraud/Non_Fraud record counts\n",
    "print(df['Class'].value_counts())\n",
    "(df.groupby('Class')['Class'].count()/df['Class'].count()) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Correlation Matrix\n",
    "cor = df.corr()\n",
    "plt.figure(figsize=(24,18))\n",
    "sns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will observe the distribution of our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=df['Class'].value_counts()\n",
    "normal_share=classes[0]/df['Class'].count()*100\n",
    "fraud_share=classes[1]/df['Class'].count()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(df['Class'])\n",
    "plt.title(\"Class Count\", fontsize=18)\n",
    "plt.xlabel(\"Record counts by class\", fontsize=15)\n",
    "plt.ylabel(\"Count\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to observe the distribution of classes with time\n",
    "plt.figure(figsize=(10,3))\n",
    "cmap = sns.color_palette('Set2')\n",
    "sns.scatterplot(x=df['Time'], y='Class', palette=cmap, data=df)\n",
    "plt.xlabel('Time', size=18)\n",
    "plt.ylabel('Class', size=18)\n",
    "plt.tick_params(axis='x', labelsize=16)\n",
    "plt.tick_params(axis='y', labelsize=16) \n",
    "plt.title('Time vs Class Distribution', size=20, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to observe the distribution of classes with Amount\n",
    "plt.figure(figsize=(10,3))\n",
    "sns.scatterplot(x=df['Amount'], y='Class', palette=cmap, data=df)\n",
    "plt.xlabel('Time', size=18)\n",
    "plt.ylabel('Class', size=18)\n",
    "plt.tick_params(axis='x', labelsize=16)\n",
    "plt.tick_params(axis='y', labelsize=16) \n",
    "plt.title('Time vs Class Distribution', size=20, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop('Time', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df['Class']\n",
    "X = df.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preserve X_test & y_test to evaluate on the test data once you build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y))\n",
    "print(np.sum(y_train))\n",
    "print(np.sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the distribution of a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the histogram of a variable from the dataset to see the skewness\n",
    "normal_records = df.Class == 0\n",
    "fraud_records = df.Class == 1\n",
    "cols = list(X.columns.values)\n",
    "\n",
    "plt.figure(figsize=(20, 60))\n",
    "for n, col in enumerate(cols):\n",
    "    plt.subplot(10,3,n+1)\n",
    "    sns.distplot(X[col][normal_records], color='green')\n",
    "    sns.distplot(X[col][fraud_records], color='red')\n",
    "    plt.title(col, fontsize=17)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is skewness present in the distribution use:\n",
    "- <b>Power Transformer</b> package present in the <b>preprocessing library provided by sklearn</b> to make distribution more gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "pt = PowerTransformer()\n",
    "pt.fit(X_train)                       ## Fit the PT on training data\n",
    "X_train_pt = pt.transform(X_train)    ## Then apply on all data\n",
    "X_test_pt = pt.transform(X_test)\n",
    "\n",
    "X_train_pt_df = pd.DataFrame(data=X_train_pt, columns=cols)\n",
    "X_test_pt_df = pd.DataFrame(data=X_test_pt, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of a variable from the dataset again to see the result \n",
    "plt.figure(figsize=(20, 60))\n",
    "for n, col in enumerate(cols):\n",
    "    plt.subplot(10,3,n+1)\n",
    "    sns.distplot(X_train_pt_df[col][normal_records], color='green')\n",
    "    sns.distplot(X_train_pt_df[col][fraud_records], color='red')\n",
    "    plt.title(col, fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of a variable from the test dataset again to see the result \n",
    "plt.figure(figsize=(20, 60))\n",
    "for n, col in enumerate(cols):\n",
    "    plt.subplot(10,3,n+1)\n",
    "    sns.distplot(X_test_pt_df[col][normal_records], color='green')\n",
    "    sns.distplot(X_test_pt_df[col][fraud_records], color='red')\n",
    "    plt.title(col, fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "- Build different models on the imbalanced dataset and see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to Store Model Performance Results\n",
    "overall_results = []\n",
    "\n",
    "# Creating function to display ROC-AUC score, f1 score and classification report\n",
    "def display_scores(y_test, y_pred, y_test_pred_proba):\n",
    "    '''\n",
    "    Display ROC-AUC score, f1 score and classification report of a model.\n",
    "    '''\n",
    "    f1score = f1_score(y_test, y_pred)\n",
    "    print(f\"F1 Score: {round(f1score*100, 2)}%\") \n",
    "    print(f\"Classification Report: \\n {classification_report(y_test, y_pred)}\")\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_pred_proba,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score(y_test, y_test_pred_proba)\n",
    "    print(f\"AUC Score: {round(auc_score*100, 2)}%\")\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    result = [f1score, auc_score]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cv_logistic_regression(X_train, y_train):\n",
    "    # Logistic Regression parameters for K-fold cross vaidation\n",
    "    params = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=4)\n",
    "\n",
    "    #perform cross validation\n",
    "    model_cv = GridSearchCV(estimator = LogisticRegression(),\n",
    "                            param_grid = params, \n",
    "                            scoring= 'roc_auc', \n",
    "                            cv = folds, \n",
    "                            n_jobs=-1,\n",
    "                            verbose = 1,\n",
    "                            return_train_score=True) \n",
    "    #perform hyperparameter tuning\n",
    "    model_cv.fit(X_train, y_train)\n",
    "    #print the evaluation result by choosing a evaluation metric\n",
    "    print('Best ROC AUC score: ', model_cv.best_score_)\n",
    "    #print the optimum value of hyperparameters\n",
    "    print('Best hyperparameters: ', model_cv.best_params_)\n",
    "    \n",
    "    cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "    \n",
    "    # plot of C versus train and validation scores\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(cv_results['param_C'], cv_results['mean_test_score'])\n",
    "    plt.plot(cv_results['param_C'], cv_results['mean_train_score'])\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('sensitivity')\n",
    "    plt.legend(['test result', 'train result'], loc='upper left')\n",
    "    plt.xscale('log')\n",
    "    plt.show()\n",
    "    return model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_logistic_regression(X_train, y_train):\n",
    "    best_params = cv_logistic_regression(X_train, y_train)\n",
    "    \n",
    "    # Instantiating the model with best C\n",
    "    log_reg_imb_model = LogisticRegression(C=best_params.get('C'))\n",
    "    \n",
    "    # Fitting the model on train dataset\n",
    "    log_reg_imb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on the train set\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'-'*10} Predict on Train Set {'-'*10}\")\n",
    "    print(\"-\" * 40)\n",
    "    y_train_pred = log_reg_imb_model.predict(X_train)\n",
    "    \n",
    "    # Predicted probability\n",
    "    y_train_pred_proba = log_reg_imb_model.predict_proba(X_train)[:,1]\n",
    "\n",
    "    result_train = display_scores(y_train, y_train_pred, y_train_pred_proba)\n",
    "    \n",
    "    # Making prediction on the test set\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"{'-'*10} Predict on Test Set {'-'*10}\")\n",
    "    print(\"-\" * 45)\n",
    "    y_test_pred = log_reg_imb_model.predict(X_test)\n",
    "\n",
    "    # Predicted probability\n",
    "    y_test_pred_proba = log_reg_imb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    result_test = display_scores(y_test, y_test_pred, y_test_pred_proba)\n",
    "    \n",
    "    print(f\"{'='*43}\\n{'='*15}Model Summary{'='*15}\\n{'='*43}\")\n",
    "    print(f\"Train Set:\\n---------\\nF1 Score - {round(result_train['F1 Score']*100, 2)}%\\nAUC Score - {round(result_train['AUC Score']*100, 2)}% \")\n",
    "    print(f\"Test Set:\\n--------\\nF1 Score - {round(result_test['F1 Score']*100, 2)}%\\nAUC Score - {round(result_test['AUC Score']*100, 2)}% \")\n",
    "    \n",
    "    return log_reg_imb_model, result_train, result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_reg_imb_model, result_train, result_test = model_logistic_regression(X_train, y_train)\n",
    "overall_results.extend[ ['Imbalanced', 'Logistic regression', 'train', *result_train],\n",
    "                        ['Imbalanced', 'Logistic regression', 'test', *result_test]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly explore other algorithms by building models like:\n",
    "- KNN\n",
    "- SVM\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation with K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_svm(X_train, y_train):\n",
    "    # Hyperparameters for Grid Search\n",
    "    param_grid = {'C': [0.1],# 10, 100], \n",
    "                  'kernel': ['sigmoid']#'rbf', \n",
    "                 } \n",
    "  \n",
    "    svm_model = SVC(probability=True, gamma='auto')\n",
    "    \n",
    "    folds = 2\n",
    "\n",
    "    #perform cross validation\n",
    "    model_cv = GridSearchCV(estimator = svm_model, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds,\n",
    "                        verbose = 3,\n",
    "                        return_train_score=True)  \n",
    "    # fit the model\n",
    "    model_cv.fit(X_train, y_train)\n",
    "    #print the evaluation result by choosing a evaluation metric\n",
    "    print('Best ROC AUC score: ', model_cv.best_score_)\n",
    "    #print the optimum value of hyperparameters\n",
    "    print('Best hyperparameters: ', model_cv.best_params_)\n",
    "    print('Best Estimator: ', model_cv.best_estimator_)\n",
    "\n",
    "    return model_cv.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_svm(X_train, y_train):\n",
    "    best_params = cv_svm(X_train, y_train)\n",
    "    \n",
    "    # fit model on training data\n",
    "    imb_model = SVC(C=best_params['C'],\n",
    "                        gamma='auto', \n",
    "                        kernel=best_params['kernel'],\n",
    "                        probability=True)\n",
    "    \n",
    "    # Fitting the model on train dataset\n",
    "    imb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on the train set\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'-'*10} Predict on Train Set {'-'*10}\")\n",
    "    print(\"-\" * 40)\n",
    "    y_train_pred = imb_model.predict(X_train)\n",
    "    \n",
    "    # Predicted probability\n",
    "    y_train_pred_proba = imb_model.predict_proba(X_train)[:,1]\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    result_train = display_scores(y_train, y_train_pred, y_train_pred_proba)\n",
    "    \n",
    "    # Making prediction on the test set\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"{'-'*10} Predict on Test Set {'-'*10}\")\n",
    "    print(\"-\" * 45)\n",
    "    y_test_pred = imb_model.predict(X_test)\n",
    "\n",
    "    # Predicted probability\n",
    "    y_test_pred_proba = imb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    result_test = display_scores(y_test, y_test_pred, y_test_pred_proba)\n",
    "    \n",
    "    print(f\"{'='*43}\\n{'='*15}Model Summary{'='*15}\\n{'='*43}\")\n",
    "    print(f\"Train Set:\\n---------\\nF1 Score - {round(result_train['F1 Score']*100, 2)}%\\nAUC Score - {round(result_train['AUC Score']*100, 2)}% \")\n",
    "    print(f\"Test Set:\\n--------\\nF1 Score - {round(result_test['F1 Score']*100, 2)}%\\nAUC Score - {round(result_test['AUC Score']*100, 2)}% \")\n",
    "    \n",
    "    result = {'SVM': {'Train':result_train, 'Test': result_test}}\n",
    "\n",
    "    return imb_model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_imb_model, result_train, result_test = model_svm(X_train, y_train)\n",
    "overall_results.extend[ ['Imbalanced', 'SVM', 'train', *result_train],\n",
    "                        ['Imbalanced', 'SVM', 'test', *result_test]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_xgboost(X_train, y_train):\n",
    "    # Hyperparameters for Grid Search\n",
    "    param_grid = {'learning_rate': [0.2, 0.6], \n",
    "             'subsample': [0.3, 0.6, 0.9]}\n",
    "    folds = 3\n",
    "\n",
    "    xgb_model = XGBClassifier(max_depth=2, n_estimators=200, eval_metric='logloss')\n",
    "    \n",
    "    #perform cross validation\n",
    "    model_cv = GridSearchCV(estimator = xgb_model, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)  \n",
    "    # fit the model\n",
    "    model_cv.fit(X_train, y_train)\n",
    "    #print the evaluation result by choosing a evaluation metric\n",
    "    print('Best ROC AUC score: ', model_cv.best_score_)\n",
    "    #print the optimum value of hyperparameters\n",
    "    print('Best hyperparameters: ', model_cv.best_params_)\n",
    "    print('Best Estimator: ', model_cv.best_estimator_)\n",
    "\n",
    "    return model_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_xgboost(X_train, y_train):\n",
    "    best_params = cv_xgboost(X_train, y_train)\n",
    "    \n",
    "    params = {\n",
    "             }\n",
    "    \n",
    "    # fit model on training data\n",
    "    xgb_imb_model = XGBClassifier(learning_rate=best_params['learning_rate'],\n",
    "                                  max_depth= 2, \n",
    "                                  n_estimators=200,\n",
    "                                  subsample=best_params['subsample'],\n",
    "                                  objective='binary:logistic',\n",
    "                                  eval_metric='logloss')\n",
    "    \n",
    "    # Fitting the model on train dataset\n",
    "    xgb_imb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on the train set\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'-'*10} Predict on Train Set {'-'*10}\")\n",
    "    print(\"-\" * 40)\n",
    "    y_train_pred = xgb_imb_model.predict(X_train)\n",
    "    \n",
    "    # Predicted probability\n",
    "    y_train_pred_proba = xgb_imb_model.predict_proba(X_train)[:,1]\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    result_train = display_scores(y_train, y_train_pred, y_train_pred_proba)\n",
    "    \n",
    "    # Making prediction on the test set\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"{'-'*10} Predict on Test Set {'-'*10}\")\n",
    "    print(\"-\" * 45)\n",
    "    y_test_pred = xgb_imb_model.predict(X_test)\n",
    "\n",
    "    # Predicted probability\n",
    "    y_test_pred_proba = xgb_imb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    result_test = display_scores(y_test, y_test_pred, y_test_pred_proba)\n",
    "    \n",
    "    print(f\"{'='*43}\\n{'='*15}Model Summary{'='*15}\\n{'='*43}\")\n",
    "    print(f\"Train Set:\\n---------\\nF1 Score - {round(result_train['F1 Score']*100, 2)}%\\nAUC Score - {round(result_train['AUC Score']*100, 2)}% \")\n",
    "    print(f\"Test Set:\\n--------\\nF1 Score - {round(result_test['F1 Score']*100, 2)}%\\nAUC Score - {round(result_test['AUC Score']*100, 2)}% \")\n",
    "    \n",
    "    result = {'XGBoost': {'Train':result_train, 'Test': result_test}}\n",
    "\n",
    "    return xgb_imb_model, result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_imb_model, result_train, result_test = model_xgboost(X_train, y_train)\n",
    "\n",
    "overall_results.extend[ ['Imbalanced', 'XGBoost', 'train', *result_train],\n",
    "                        ['Imbalanced', 'XGBoost', 'test', *result_test]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proceed with the model which shows the best result \n",
    "- Apply the best hyperparameter on the model\n",
    "- Predict on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb_imb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the important features of the best model to understand the dataset\n",
    "- This will not give much explanation on the already transformed dataset\n",
    "- But it will help us in understanding if the dataset is not PCA transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp = []\n",
    "for i in clf.feature_importances_:\n",
    "    var_imp.append(i)\n",
    "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
    "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
    "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
    "\n",
    "# Variable on Index-16 and Index-13 seems to be the top 2 variables\n",
    "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
    "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "np.random.shuffle(X_train_0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
    "            label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building with balancing Classes\n",
    "\n",
    "##### Perform class balancing with :\n",
    "- Random Oversampling\n",
    "- SMOTE\n",
    "- ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_train_over, y_train_over = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# Befor sampling class distribution\n",
    "print('Before sampling class distribution:-',Counter(y_train))\n",
    "# new class distribution \n",
    "print('New class distribution:-',Counter(y_train_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building with Balanced Dataset (Random Oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_over_model, result_train, result_test = model_logistic_regression(X_train_over, y_train_over)\n",
    "\n",
    "overall_results.extend[ ['Random Oversampling', 'Logistic Regression', 'train', *result_train],\n",
    "                        ['Random Oversampling', 'Logistic Regression', 'test', results['test']['f1score'], results['test']['auc_score']]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_over_model, result_train, result_test = model_svm(X_train_over, y_train_over)\n",
    "\n",
    "overall_results.extend[ ['Random Oversampling', 'SVM', 'train', *result_train],\n",
    "                        ['Random Oversampling', 'SVM', 'test', *result_test]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_over_model, result_train, result_test = model_xgboost(X_train_over, y_train_over)\n",
    "\n",
    "overall_results.extend[ ['Random Oversampling', 'XGBoost', 'train', *result_train],\n",
    "                        ['Random Oversampling', 'XGBoost', 'test', *result_test]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE (Synthetic Minority Oversampling Technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the class distribution after applying SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=0)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
    "# Artificial minority samples and corresponding minority labels from SMOTE are appended\n",
    "# below X_train and y_train respectively\n",
    "# So to exclusively get the artificial minority samples from SMOTE, we do\n",
    "X_train_smote_1 = X_train_smote[X_train.shape[0]:]\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_smote_1.iloc[:X_train_1.shape[0], 0], X_train_smote_1.iloc[:X_train_1.shape[0], 1],\n",
    "            label='Artificial SMOTE Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building with Balanced Dataset (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_smote_model, result_train, result_test = model_logistic_regression(X_train_smote, y_train_smote)\n",
    "\n",
    "overall_results.extend[ ['SMOTE', 'Logistic regression', 'train', *result_train],\n",
    "                        ['SMOTE', 'Logistic regression', 'test', *result_test]\n",
    "                      ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_smote_model, result_train, result_test = model_svm(X_train_smote, y_train_smote)\n",
    "\n",
    "overall_results.extend[ ['SMOTE', 'SVM', 'train', *result_train],\n",
    "                        ['SMOTE', 'SVM', 'test', *result_test]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_smote_model, result_train, result_test = model_xgboost(X_train_smote, y_train_smote)\n",
    "\n",
    "overall_results.extend[ ['SMOTE', 'XGBoost', 'train', *result_train],\n",
    "                        ['SMOTE', 'XGBoost', 'test', *result_test]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN (Adaptive Synthetic Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the class distribution after applying ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = ADASYN(random_state=0)\n",
    "X_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train)\n",
    "\n",
    "# Artificial minority samples and corresponding minority labels from ADASYN are appended\n",
    "# below X_train and y_train respectively\n",
    "# So to exclusively get the artificial minority samples from ADASYN, we do\n",
    "X_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:]\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_adasyn_1.iloc[:X_train_1.shape[0], 0], X_train_adasyn_1.iloc[:X_train_1.shape[0], 1],\n",
    "            label='Artificial ADASYN Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building with Balanced Dataset (ADASYN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_adasyn_model, result_train, result_test = model_logistic_regression(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "overall_results.extend[ ['ADASYN', 'Linear Regression', 'train', results['train']['f1score'], results['train']['auc_score']],\n",
    "                        ['ADASYN', 'Linear Regression', 'test', results['test']['f1score'], results['test']['auc_score']]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_adasyn_model, result_train, result_test = model_svm(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "overall_results.extend[ ['ADASYN', 'SVM', 'train', *result_train],\n",
    "                        ['ADASYN', 'SVM', 'test', *result_test]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_adasyn_model, result_train, result_test = model_xgboost(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "overall_results.extend[ ['ADASYN', 'XGBoost', 'train', *result_train],\n",
    "                        ['ADASYN', 'XGBoost', 'test', *result_test]\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the oversampling method which shows the best result on a model\n",
    "- Apply the best hyperparameter on the model\n",
    "- Predict on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the best oversampling method on X_train & y_train\n",
    "\n",
    "clf = xgb_adasyn_model\n",
    "#clf.fit( ) # fit on the balanced dataset\n",
    "#print() --> #print the evaluation score on the X_test by choosing the best evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the important features of the best model to understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp = []\n",
    "for i in clf.feature_importances_:\n",
    "    var_imp.append(i)\n",
    "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
    "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
    "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
    "\n",
    "# Variable on Index-13 and Index-9 seems to be the top 2 variables\n",
    "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
    "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "np.random.shuffle(X_train_0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
    "            label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overall_results)\n",
    "print(pd.DataFrame(overall_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
